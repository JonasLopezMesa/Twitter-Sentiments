{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST API Search Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método de consulta de tweets históricos (hasta una semana) basándose en varios criterios, es muy adecuado para un análisis estático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga inicial de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Servicio\n",
      "[nltk_data]     Técnico\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Servicio\n",
      "[nltk_data]     Técnico\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pymongo import MongoClient\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "#para los otros tipos de algoritmos:\n",
    "from sklearn.svm import SVC #Support Vector Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#########################################\n",
    "#Para importar y exportar el algoritmo entrenado\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "#########################################\n",
    "import csv\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import collections.abc\n",
    "import collections\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "consumer_key='ynSB0dFvqPl3xRU7AmYk39rGT'\n",
    "consumer_secret='6alIXTKSxf0RE57QK3fDQ8dxdvlsVr1IRsHDZmoSlMx96YKBFD'\n",
    "access_token='966591013182722049-BVXW14Hf5s6O2oIwS3vtJ3S3dOsKLbY'\n",
    "access_token_secret='829DTKPjmwsSytmp1ky9fMCJkjV0LZ04TbL9oqHGV6cDm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte en la que se importa las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'premier league -filter:retweets AND -filter:replies'\n",
    "url = 'https://api.Twitter.com/1.1/search/tweets.json'\n",
    "pms = {'q' : q, 'count' : 100, 'lang' : 'en', 'result_type': 'recent'} \n",
    "auth = OAuth1(consumer_key, consumer_secret, access_token,access_token_secret)\n",
    "#res = requests.get(url, params = pms, auth=auth)\n",
    "#esta parte del código se utiliza realmente en la paginación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sección de código que realiza la consulta **q** mediante la api de twitter utilizando los parámetros **pms**. Después, se crea una variable **auth** que corresponde a los datos exigidos por la API para conectarse que son *Las claves y los tokens de acceso.*\n",
    "Por último se realiza la petición utilizando todos los datos necesarios por medio de una función de la librería requests **(request.get)** el resultado se vuelca en la variable **res**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = res.json()\n",
    "#esta parte del código se utiliza realmente en la paginación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la conección es correcta se recuperarán varios documentos que se pueden convertir a json. En el formato json es en el cual extraeremos la información que nos importa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos para más de 100 tweets (Paginación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como twitter te permite obtener hasta 100 tweets por llamada, si queremos recuperar más, necesitamos recordar los IDs de los tweets ya descargados para no repetir tweets en las siguientes llamadas. Esto es la paginación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"baseDeDatos\"\n",
    "collection_name = \"coleccion\"\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client[database_name]\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MongoDB** *es una base de datos NoSQL ágil orientado a documentos, esto significa que en lugar de guardar los datos en tablas como se hace en las bbdd relacionales, MongoDB guarda estructuras de datos en documentos similares a JSON con un esquema dinámico.*\n",
    "En nuestro caso importamos la librería para usar el cliente y creamos una base de datos y una colección a raíz de la base de datos. Ya cambiaré los nombres de **database_name** y **collection_name**. <font color='red'>falta establecer el nombre de la base de datos y el nombre de la colección</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection status: OK\n"
     ]
    }
   ],
   "source": [
    "pages_counter = 0\n",
    "number_of_pages = 1\n",
    "while pages_counter < number_of_pages:\n",
    "    pages_counter += 1\n",
    "    res = requests.get(url, params = pms, auth=auth)\n",
    "    print(\"Connection status: %s\" % res.reason)\n",
    "    tweets = res.json()\n",
    "    ids = [i['id'] for i in tweets['statuses']]\n",
    "    pms['max_id'] = min(ids) - 1\n",
    "    collection.insert_many(tweets['statuses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código para la paginación consiste en un bucle que se repetirá tantas veces como el número aproximado de tweets que queramos recuperar de twitter. Dentro del bucle se realiza la *consulta* (**requests.get**) y se guarda en la variable tweets con formato json (igual que se hacía antes de la paginación)\n",
    "Después, para implementar lo que es realmente la paginación; \n",
    "- Se recolectan todos los **ids** de los tweets recogidos mediante el **get**.\n",
    "- Se almacena en **i** todos los **ids** de todos los tweets.\n",
    "- Se establece en **pms** como el *id mayor*, el *id mínimo* de **i** -1 (porque se incluiría y después se duplica)\n",
    "- Por último se inserta en la base de datos (Con **insert_many**) todos los tweets que han sido recuperados en esta *ronda*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#client = MongoClient('mongodb://localhost:27017/')\n",
    "#db = client['db']\n",
    "#collection = db['collection']\n",
    "documents = []\n",
    "for doc in collection.find():\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que se hace a continuación es utilizar la base de datos de mongo en la que se ha estado almacenando los tweets para volcar la información de forma tabular, donde las columnas indican los nombres de los nodos del documento y las filas representan los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso se crea un Dataframe usando la lista de documentos que hemos construido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos gustaría analizar los sentimientos de los tweets que son escritos por la gente desde sus diferentes dispositivos. Por lo tanto, debemos deshacernos de los tweets compuestos por bots, páginas web, servicios de envío automatizado, etc. No podemos identificar esos tweets al 100%, pero es bastante buena suposición sería seleccionar tweets publicados desde dispositivos físicos, es decir, iPhones, Teléfonos Android, ordenadores de sobremesa y portátiles. \n",
    "Los documentos de Twitter tienen una característica interesante, ya que mantienen la información sobre la fuente de la creación del tweet. Cada vez que alguien usa un dispositivo para componer un tweet, el se mantiene la información al respecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Anaconda\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "df['tweet_source'] = df['source'].apply(lambda x: BeautifulSoup(x).get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta linea de código vamos a crear una nueva columna en el data Frame df que se llama tweet_source y ahí vamos a poner lo que había en la columna source pero con la información que necesitamos, es decir, la fuente del tweet.\n",
    "Ahora bien, sólo nos interesan los dispositivos. La información sobre los dispositivos es muy clara: encontraremos fuentes de tweets como Twitter para iPhone, Twitter para Android, Twitter Web Client, Twitter para BlackBerry, Twitter para Mac, Twitter para Windows, etc. Los nombres de los dispositivos comienzan con la palabra \"Twitter\". Utilizaremos esta propiedad para abordar nuestra segunda cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = list(set(df[df['tweet_source'].str.startswith('Twitter')]['tweet_source']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera se extrae en la variable devices todas las fuentes de los tweets que comiencen por Twitter, es decir, todas las que vienen de dispositivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices.remove('Twitter Ads')\n",
    "df = df[df['tweet_source'].isin(devices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitamos de la lista de dispositivos todos aquellos que sean anuncios de Twitter y dejamos en el data frame sólamente aquellos twits cuya fuente sea la misma que la que tenemos en devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['text'].str.contains(\"Ghana|ghana|jamaica|Jamaica|Ladbrokes|India|Pa kistan|Ghana Premier League|Vijay|Predictions|Egyptian Premier League|cricket|Kings|Caribbean Premier League|@cricbuzz|Cricinfo\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto lo que se hace en el libro (quizá yo no tenga que hacerlo) es eliminar la posibilidad de que salgan twits de Premiere League que no sea la inglesa. Hay otras Premiere League que no interesa para el estudio del libro y se quita mediante esta función utilizando las palabras clave que necesitemos. Toda esta búsqueda para la limpieza se realiza en el campo text de df (df.text)\n",
    "El código dice, en esencia, mantiene todas las filas que NO (~) tengan en la columa \"text\" las siguientes palabras (| es el operador lógico OR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION\n",
    "df['tokens'] = df['text'].apply(TweetTokenizer().tokenize)\n",
    "#STOPWORDS\n",
    "stopwords_vocabulary = stopwords.words('english') #estará en español?\n",
    "df['stopwords'] = df['tokens'].apply(lambda x: [i for i in x if i.lower() not in stopwords_vocabulary])\n",
    "#SPECIAL CHARACTERS AND STOPWORDS REMOVAL\n",
    "punctuations = list(string.punctuation)\n",
    "df['punctuation'] = df['stopwords'].apply(lambda x: [i for i in x if i not in punctuations])\n",
    "df['digits'] = df['punctuation'].apply(lambda x: [i for i in x if i[0] not in list(string.digits)])\n",
    "df['final'] = df['digits'].apply(lambda x: [i for i in x if len(i) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Sentimientos\n",
    "Utilizaremos VADER para analizar los sentimientos de los datos que hemos recibido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos las librerías que necesitamos\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "df['sentiment'] = df.text.apply(lambda x: sentiment.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21c0cb32e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos = len(df[df.sentiment > 0]) #guardamos la cantidad de tweets positivos\n",
    "neg = len(df[df.sentiment < 0]) #guardamos la cantidad de tweets negativos\n",
    "neu = len(df[df.sentiment == 0]) #guardamos la cantidad de tweets neutros\n",
    "y = [pos, neu, neg] # vector de la cantidad de tweets positivos, negativos y neutros\n",
    "#construimos un gráfico con los datos del vector\n",
    "plt.title(\"Sentiment Analysis\")\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xticks(range(len(y)), ['positive', 'neutral', 'negative'])\n",
    "plt.bar(range(len(y)), height=y, width = 0.75, align = 'center', alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etiquetado de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Carga de archivos realizada\n",
      "2. Limpieza del dataset realizada\n",
      "3. Limpieza del dataset2 realizada\n"
     ]
    }
   ],
   "source": [
    "classes = ['4', '2', '0']\n",
    "\n",
    "filename = 'testdata.manual.2009.06.14.csv'\n",
    "filename2 = 'prueba.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "tweetys = dataset['final']\n",
    "prueba = pd.read_csv(filename) #para la última parte del NER combinado\n",
    "dataset2 = pd.read_csv(filename2)\n",
    "print(\"1. Carga de archivos realizada\")\n",
    "\n",
    "#LIMPIEZA DE DATOS DE DATASET\n",
    "#TOKENIZATION\n",
    "dataset['tokens'] = dataset['final'].apply(TweetTokenizer().tokenize)\n",
    "#STOPWORDS\n",
    "stopwords_vocabulary = stopwords.words('english') #estará en español?\n",
    "dataset['stopwords'] = dataset['tokens'].apply(lambda x: [i for i in x if i.lower() not in stopwords_vocabulary])\n",
    "#SPECIAL CHARACTERS AND STOPWORDS REMOVAL\n",
    "punctuations = list(string.punctuation)\n",
    "dataset['punctuation'] = dataset['stopwords'].apply(lambda x: [i for i in x if i not in punctuations])\n",
    "dataset['digits'] = dataset['punctuation'].apply(lambda x: [i for i in x if i[0] not in list(string.digits)])\n",
    "dataset['final'] = dataset['digits'].apply(lambda x: [i for i in x if len(i) > 1])\n",
    "print(\"2. Limpieza del dataset realizada\")\n",
    "\n",
    "#LIMPIEZA DE DATOS DE DATASET2\n",
    "#TOKENIZATION\n",
    "dataset2['tokens'] = dataset2['final'].apply(TweetTokenizer().tokenize)\n",
    "#STOPWORDS\n",
    "stopwords_vocabulary = stopwords.words('english') #estará en español?\n",
    "dataset2['stopwords'] = dataset2['tokens'].apply(lambda x: [i for i in x if i.lower() not in stopwords_vocabulary])\n",
    "#SPECIAL CHARACTERS AND STOPWORDS REMOVAL\n",
    "punctuations = list(string.punctuation)\n",
    "dataset2['punctuation'] = dataset2['stopwords'].apply(lambda x: [i for i in x if i not in punctuations])\n",
    "dataset2['digits'] = dataset2['punctuation'].apply(lambda x: [i for i in x if i[0] not in list(string.digits)])\n",
    "dataset2['final'] = dataset2['digits'].apply(lambda x: [i for i in x if len(i) > 1])\n",
    "print(\"3. Limpieza del dataset2 realizada\")\n",
    "\n",
    "#Aquí es el lugar donde defino el número de tweets que usaré en los modelos siempre con el porcentaje 80:20\n",
    "train_data = dataset2['final'][0:300]\n",
    "train_labels = dataset2['label'][0:300]\n",
    "\n",
    "test_data = dataset['final'][0:75]\n",
    "test_labels = dataset['label'][0:75]\n",
    "\n",
    "train_data = list(train_data.apply(' '.join))\n",
    "test_data = list(test_data.apply(' '.join))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de los datos para los modelos\n",
    "se prepara la información para dejarla de manera adecuada lista para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=True)\n",
    "#min_df = 5. Descarta palabras que aparezcan en menos de 5 tweets.\n",
    "#max_df = 0.8 Descarta palabras que aparezcan en más del 80% de los tweets\n",
    "#sublinear_tf = True Le da una importancia logarítmica a la repetición de palabras\n",
    "#use_idf = True Permite la frecuencia inversa del documento ???\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para Navie Bayes\n",
    "Según el ejemplo del libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "#nb.fit(train_vectors, train_labels).score(test_vectors, test_labels)\n",
    "#Para guardar el entrenamiento del modelo\n",
    "#file = open('NaiveBayes', 'wb')\n",
    "#dump(nb, file)\n",
    "#Para cargar el entrenamiento del modelo\n",
    "file = open('NaiveBayes', 'rb')\n",
    "nb = load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.43      0.34        21\n",
      "          2       0.19      0.36      0.24        14\n",
      "          4       0.69      0.28      0.39        40\n",
      "\n",
      "avg / total       0.48      0.33      0.35        75\n",
      "\n",
      "[[ 9 10  2]\n",
      " [ 6  5  3]\n",
      " [17 12 11]]\n",
      "Cross validation 0.56\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes\")\n",
    "print(classification_report(test_labels, nb.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, nb.predict(test_vectors)))\n",
    "predicted = cross_val_predict(nb, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msvc = SVC()\n",
    "msvc.fit(train_vectors, train_labels).score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      1.00      0.44        21\n",
      "          2       0.00      0.00      0.00        14\n",
      "          4       0.00      0.00      0.00        40\n",
      "\n",
      "avg / total       0.08      0.28      0.12        75\n",
      "\n",
      "[[21  0  0]\n",
      " [14  0  0]\n",
      " [40  0  0]]\n",
      "Cross validation 0.5333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVC\")\n",
    "print(classification_report(test_labels, msvc.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, msvc.predict(test_vectors)))\n",
    "predicted = cross_val_predict(msvc, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier()\n",
    "neigh.fit(train_vectors, train_labels).score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.52      0.32        21\n",
      "          2       0.15      0.21      0.18        14\n",
      "          4       0.50      0.10      0.17        40\n",
      "\n",
      "avg / total       0.36      0.24      0.21        75\n",
      "\n",
      "[[11  8  2]\n",
      " [ 9  3  2]\n",
      " [27  9  4]]\n",
      "Cross validation 0.37333333333333335\n"
     ]
    }
   ],
   "source": [
    "print(\"KNeighbors Classifier\")\n",
    "print(classification_report(test_labels, neigh.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, neigh.predict(test_vectors)))\n",
    "predicted = cross_val_predict(neigh, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para DessisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3466666666666667"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(train_vectors, train_labels).score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.33      0.40        21\n",
      "          2       0.20      0.57      0.29        14\n",
      "          4       0.55      0.28      0.37        40\n",
      "\n",
      "avg / total       0.47      0.35      0.36        75\n",
      "\n",
      "[[ 7 10  4]\n",
      " [ 1  8  5]\n",
      " [ 6 23 11]]\n",
      "Cross validation 0.49333333333333335\n"
     ]
    }
   ],
   "source": [
    "print(\"DecisionTreeClassifier\")\n",
    "print(classification_report(test_labels, tree.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, tree.predict(test_vectors)))\n",
    "predicted = cross_val_predict(tree, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para MLP Perceptrion Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(train_vectors, train_labels).score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.29      0.36        21\n",
      "          2       0.19      0.57      0.29        14\n",
      "          4       0.52      0.28      0.36        40\n",
      "\n",
      "avg / total       0.45      0.33      0.35        75\n",
      "\n",
      "[[ 6 10  5]\n",
      " [ 1  8  5]\n",
      " [ 5 24 11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 0.5066666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"MLPClassifier\")\n",
    "print(classification_report(test_labels, mlp.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, mlp.predict(test_vectors)))\n",
    "predicted = cross_val_predict(mlp, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Name Entity Recognition\n",
    "En la siguiente sección implementamos el reconocimiento de entidades. Es muy lento, así que vamos a tener que ver cómo acelerar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('GM', 20),\n",
       " ('Time', 20),\n",
       " ('Warner', 20),\n",
       " ('Nike', 16),\n",
       " ('Google', 11),\n",
       " ('Stanford', 10),\n",
       " ('Safeway', 6),\n",
       " ('Canon', 5),\n",
       " ('New', 5),\n",
       " ('AIG', 4)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = StanfordNERTagger(r'C:\\Users\\Servicio Técnico\\Documents\\stanford-ner-2018-02-27\\classifiers\\english.all.3class.distsim.crf.ser.gz')\n",
    "entities = []\n",
    "\n",
    "for r in tweetys:\n",
    "    #print(\"está analizando(r): \", r)\n",
    "    lst_tags = st.tag(r) #no tengo que hacer el split porque ya está hecho?\n",
    "    for tup in lst_tags:\n",
    "        #print(\"está analizando(tup): \", tup)\n",
    "        if(tup[1] != 'O'):\n",
    "            #print(\"mete(tup) \", tup, \"en las entidades\")\n",
    "            entities.append(tup)\n",
    "df_entities = pd.DataFrame(entities)\n",
    "df_entities.columns = [\"word\",\"ner\"]\n",
    "#Organizaciones\n",
    "organizations =df_entities[df_entities['ner'].str.contains(\"ORGANIZATION\")]\n",
    "cnt = Counter(organizations['word'])\n",
    "cnt.most_common(10)\n",
    "\n",
    "\n",
    "# 1. Coger datos de twitter.\n",
    "# 2. Cargar un csv con datos y verlo en una gráfica y usar NER.\n",
    "# 3. Tweet por tweet determinar de qué está hablando y el sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Obama', 11),\n",
       " ('Malcolm', 7),\n",
       " ('Warren', 7),\n",
       " ('Bobby', 7),\n",
       " ('Gladwell', 6),\n",
       " ('Flay', 6),\n",
       " ('Pelosi', 5),\n",
       " ('Buffet', 5),\n",
       " ('Kobe', 4),\n",
       " ('Booz', 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Personas\n",
    "person =df_entities[df_entities['ner'].str.contains(\"PERSON\")]\n",
    "cnt_person = Counter(person['word'])\n",
    "cnt_person.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('China', 9),\n",
       " ('San', 8),\n",
       " ('Francisco', 8),\n",
       " ('North', 6),\n",
       " ('Korea', 6),\n",
       " ('Iran', 4),\n",
       " ('Bay', 2),\n",
       " ('East', 2),\n",
       " ('Palo', 2),\n",
       " ('Alto', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Localizaciones\n",
    "locations =df_entities[df_entities['ner'].str.contains(\"LOCATION\")]\n",
    "cnt_location = Counter(locations['word'])\n",
    "cnt_location.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinando NER con el análisis de sentimientos.\n",
    "Lo que vamos a hacer ahora es combinar el reconocimiento de entidades con el análisis de sentimientos. En el libro lo pone de manera explícita, pero vamos a tratar de hacerlo de manera automática.\n",
    "Es decir vamos a coger el primero de todas las organizaciones, personas y lugares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.142857142857143"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = prueba[prueba['final'].str.contains(cnt_person.most_common(2)[1][0])]\n",
    "avg_sentiment = np.mean(subset['label'])\n",
    "avg_sentiment\n",
    "#subset = dataset[dataset['final'].str.contains(cnt_person.most_common(1)[0][0])]\n",
    "#avg_sentiment = np.mean(subset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
