{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST API Search Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método de consulta de tweets históricos (hasta una semana) basándose en varios criterios, es muy adecuado para un análisis estático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga inicial de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Servicio\n",
      "[nltk_data]     Técnico\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Servicio\n",
      "[nltk_data]     Técnico\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pymongo import MongoClient\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "#para los otros tipos de algoritmos:\n",
    "from sklearn.svm import SVC #Support Vector Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#########################################\n",
    "#Para importar y exportar el algoritmo entrenado\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "#########################################\n",
    "import csv\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import collections.abc\n",
    "import collections\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "consumer_key='ynSB0dFvqPl3xRU7AmYk39rGT'\n",
    "consumer_secret='6alIXTKSxf0RE57QK3fDQ8dxdvlsVr1IRsHDZmoSlMx96YKBFD'\n",
    "access_token='966591013182722049-BVXW14Hf5s6O2oIwS3vtJ3S3dOsKLbY'\n",
    "access_token_secret='829DTKPjmwsSytmp1ky9fMCJkjV0LZ04TbL9oqHGV6cDm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte en la que se importa las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'premier league -filter:retweets AND -filter:replies'\n",
    "url = 'https://api.Twitter.com/1.1/search/tweets.json'\n",
    "pms = {'q' : q, 'count' : 100, 'lang' : 'en', 'result_type': 'recent'} \n",
    "auth = OAuth1(consumer_key, consumer_secret, access_token,access_token_secret)\n",
    "#res = requests.get(url, params = pms, auth=auth)\n",
    "#esta parte del código se utiliza realmente en la paginación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sección de código que realiza la consulta **q** mediante la api de twitter utilizando los parámetros **pms**. Después, se crea una variable **auth** que corresponde a los datos exigidos por la API para conectarse que son *Las claves y los tokens de acceso.*\n",
    "Por último se realiza la petición utilizando todos los datos necesarios por medio de una función de la librería requests **(request.get)** el resultado se vuelca en la variable **res**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = res.json()\n",
    "#esta parte del código se utiliza realmente en la paginación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la conección es correcta se recuperarán varios documentos que se pueden convertir a json. En el formato json es en el cual extraeremos la información que nos importa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos para más de 100 tweets (Paginación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como twitter te permite obtener hasta 100 tweets por llamada, si queremos recuperar más, necesitamos recordar los IDs de los tweets ya descargados para no repetir tweets en las siguientes llamadas. Esto es la paginación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"baseDeDatos\"\n",
    "collection_name = \"coleccion\"\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client[database_name]\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MongoDB** *es una base de datos NoSQL ágil orientado a documentos, esto significa que en lugar de guardar los datos en tablas como se hace en las bbdd relacionales, MongoDB guarda estructuras de datos en documentos similares a JSON con un esquema dinámico.*\n",
    "En nuestro caso importamos la librería para usar el cliente y creamos una base de datos y una colección a raíz de la base de datos. Ya cambiaré los nombres de **database_name** y **collection_name**. <font color='red'>falta establecer el nombre de la base de datos y el nombre de la colección</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection status: OK\n"
     ]
    }
   ],
   "source": [
    "pages_counter = 0\n",
    "number_of_pages = 1\n",
    "while pages_counter < number_of_pages:\n",
    "    pages_counter += 1\n",
    "    res = requests.get(url, params = pms, auth=auth)\n",
    "    print(\"Connection status: %s\" % res.reason)\n",
    "    tweets = res.json()\n",
    "    ids = [i['id'] for i in tweets['statuses']]\n",
    "    pms['max_id'] = min(ids) - 1\n",
    "    collection.insert_many(tweets['statuses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código para la paginación consiste en un bucle que se repetirá tantas veces como el número aproximado de tweets que queramos recuperar de twitter. Dentro del bucle se realiza la *consulta* (**requests.get**) y se guarda en la variable tweets con formato json (igual que se hacía antes de la paginación)\n",
    "Después, para implementar lo que es realmente la paginación; \n",
    "- Se recolectan todos los **ids** de los tweets recogidos mediante el **get**.\n",
    "- Se almacena en **i** todos los **ids** de todos los tweets.\n",
    "- Se establece en **pms** como el *id mayor*, el *id mínimo* de **i** -1 (porque se incluiría y después se duplica)\n",
    "- Por último se inserta en la base de datos (Con **insert_many**) todos los tweets que han sido recuperados en esta *ronda*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#client = MongoClient('mongodb://localhost:27017/')\n",
    "#db = client['db']\n",
    "#collection = db['collection']\n",
    "documents = []\n",
    "for doc in collection.find():\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que se hace a continuación es utilizar la base de datos de mongo en la que se ha estado almacenando los tweets para volcar la información de forma tabular, donde las columnas indican los nombres de los nodos del documento y las filas representan los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso se crea un Dataframe usando la lista de documentos que hemos construido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos gustaría analizar los sentimientos de los tweets que son escritos por la gente desde sus diferentes dispositivos. Por lo tanto, debemos deshacernos de los tweets compuestos por bots, páginas web, servicios de envío automatizado, etc. No podemos identificar esos tweets al 100%, pero es bastante buena suposición sería seleccionar tweets publicados desde dispositivos físicos, es decir, iPhones, Teléfonos Android, ordenadores de sobremesa y portátiles. \n",
    "Los documentos de Twitter tienen una característica interesante, ya que mantienen la información sobre la fuente de la creación del tweet. Cada vez que alguien usa un dispositivo para componer un tweet, el se mantiene la información al respecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Anaconda\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "df['tweet_source'] = df['source'].apply(lambda x: BeautifulSoup(x).get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta linea de código vamos a crear una nueva columna en el data Frame df que se llama tweet_source y ahí vamos a poner lo que había en la columna source pero con la información que necesitamos, es decir, la fuente del tweet.\n",
    "Ahora bien, sólo nos interesan los dispositivos. La información sobre los dispositivos es muy clara: encontraremos fuentes de tweets como Twitter para iPhone, Twitter para Android, Twitter Web Client, Twitter para BlackBerry, Twitter para Mac, Twitter para Windows, etc. Los nombres de los dispositivos comienzan con la palabra \"Twitter\". Utilizaremos esta propiedad para abordar nuestra segunda cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = list(set(df[df['tweet_source'].str.startswith('Twitter')]['tweet_source']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera se extrae en la variable devices todas las fuentes de los tweets que comiencen por Twitter, es decir, todas las que vienen de dispositivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devices.remove('Twitter Ads')\n",
    "df = df[df['tweet_source'].isin(devices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitamos de la lista de dispositivos todos aquellos que sean anuncios de Twitter y dejamos en el data frame sólamente aquellos twits cuya fuente sea la misma que la que tenemos en devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['text'].str.contains(\"Ghana|ghana|jamaica|Jamaica|Ladbrokes|India|Pa kistan|Ghana Premier League|Vijay|Predictions|Egyptian Premier League|cricket|Kings|Caribbean Premier League|@cricbuzz|Cricinfo\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data=df['text'])\n",
    "df2.to_csv('export2.csv')\n",
    "datasetr = pd.read_csv('export.csv')\n",
    "df3 =pd.DataFrame(data=datasetr['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto lo que se hace en el libro (quizá yo no tenga que hacerlo) es eliminar la posibilidad de que salgan twits de Premiere League que no sea la inglesa. Hay otras Premiere League que no interesa para el estudio del libro y se quita mediante esta función utilizando las palabras clave que necesitemos. Toda esta búsqueda para la limpieza se realiza en el campo text de df (df.text)\n",
    "El código dice, en esencia, mantiene todas las filas que NO (~) tengan en la columa \"text\" las siguientes palabras (| es el operador lógico OR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION\n",
    "df['tokens'] = df['text'].apply(TweetTokenizer().tokenize)\n",
    "#STOPWORDS\n",
    "stopwords_vocabulary = stopwords.words('english') #estará en español?\n",
    "df['stopwords'] = df['tokens'].apply(lambda x: [i for i in x if i.lower() not in stopwords_vocabulary])\n",
    "#SPECIAL CHARACTERS AND STOPWORDS REMOVAL\n",
    "punctuations = list(string.punctuation)\n",
    "df['punctuation'] = df['stopwords'].apply(lambda x: [i for i in x if i not in punctuations])\n",
    "df['digits'] = df['punctuation'].apply(lambda x: [i for i in x if i[0] not in list(string.digits)])\n",
    "df['final'] = df['digits'].apply(lambda x: [i for i in x if len(i) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Sentimientos\n",
    "Utilizaremos VADER para analizar los sentimientos de los datos que hemos recibido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos las librerías que necesitamos\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "df['sentiment'] = df.text.apply(lambda x: sentiment.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1559f6c2f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos = len(df[df.sentiment > 0]) #guardamos la cantidad de tweets positivos\n",
    "neg = len(df[df.sentiment < 0]) #guardamos la cantidad de tweets negativos\n",
    "neu = len(df[df.sentiment == 0]) #guardamos la cantidad de tweets neutros\n",
    "y = [pos, neu, neg] # vector de la cantidad de tweets positivos, negativos y neutros\n",
    "#construimos un gráfico con los datos del vector\n",
    "plt.title(\"Sentiment Analysis\")\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xticks(range(len(y)), ['positive', 'neutral', 'negative'])\n",
    "plt.bar(range(len(y)), height=y, width = 0.75, align = 'center', alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etiquetado de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Carga de archivos realizada\n",
      "2. Limpieza del dataset realizada\n",
      "3. Limpieza del dataset2 realizada\n",
      "['@stellargirl loooooooovvvvvveee Kindle DX cool fantastic right', 'Reading kindle ... Love ... Lee childs good read', 'Ok first assesment #kindle2 ... fucking rocks', \"@kenburbary love Kindle I've mine months never looked back new big one huge need remorse :)\", '@mikefish Fair enough Kindle think perfect :)', \"@richardebaker big I'm quite happy Kindle\", 'Fuck economy hate aig non loan given asses', 'Jquery new best friend', 'Loves twitter', 'love Obama makes jokes', 'Check video President Obama White House Correspondents Dinner http://bit.ly/IMXUM', '@Karoli firmly believe Obama Pelosi ZERO desire civil charade slogan want destroy conservatism', 'House Correspondents dinner last night whoopi barbara sherri went Obama got standing ovation', 'Watchin Espn .. Jus seen new Nike Commerical Puppet Lebron .. sh hilarious ... LMAO', 'dear nike stop flywire shit waste science ugly love @vincentx24x', '#lebron best athlete generation time basketball related want get inter-sport debates __1', 'talking guy last night telling die hard Spurs fan also told hates LeBron James', 'love lebron http://bit.ly/PdHur', \"@ludajuice Lebron Beast I'm still cheering .. til end\", '@Pmillzz lebron BOSS', \"@sketchbug Lebron hometown hero lol love Lakers let's go Cavs lol\", 'lebron zydrunas awesome duo', '@wordwhizkid Lebron beast ... nobody NBA comes even close', 'downloading apps iphone much fun :-) literally app anything', 'good news call Visa office saying everything fine ... relief sick scams Stealing', 'http://twurl.nl/epkr4b awesome come back @biz via @fredwilson', 'montreal long weekend Much needed', 'Booz Allen Hamilton bad ass homegrown social collaboration platform Way cool #ttiv', '#MLUC09 Customer Innovation Award Winner Booz Allen Hamilton http://ping.fm/c2hPP', '@SoChi2 current use Nikon D90 love much Canon chose D90 video feature mistake', 'need suggestions good IR filter canon ... got pls DM', '@surfit checked google business blip shows second entry Huh good ba ... http://blip.fm/~6emhv', '@phyreman9 Google always good place look mentioned worked Mustang Dad @KimbleT', 'Played android google phone slide screen scares would break fucker fast Still prefer iPhone', 'US planning resume military tribunals Guantanamo Bay ... time trial AIG execs Chrysler debt holders', 'omg bored tattoooos itchy help aha =)', \"I'm itchy miserable\", \"@sekseemess I'm itchy Maybe later lol\", 'RT @jessverr love nerdy Stanford human biology videos makes miss school http://bit.ly/13t7NR', '@spinuzzi bit crazy steep learning curve LyX really good long docs anything shorter would insane', \"I'm listening Danny Gokey <3 <3 <3 Aww he's amazing <3 much :)\", 'going sleep bike ride :]', 'cant sleep ... tooth aching', 'Blah blah blah old old plans today going back sleep guess', 'glad didnt Bay Breakers today freaking degrees San Francisco wtf', 'San Francisco Bay Breakers', 'landed San Francisco', 'San Francisco today suggestions', 'Obama Administration Must Stop Bonuses AIG Ponzi Schemers ... http://bit.ly/2CUIg', 'started think Citi really deep gonna survive turmoil gonna next AIG', \"ShaunWoo hate'n AiG\", '@YarnThing regret going see Star Trek AWESOME', 'way see Star Trek Esquire', 'Going see star trek soon dad', 'annoying new trend internets people picking apart michael lewis malcolm gladwell nobody wants read', 'Bill Simmons conversation Malcolm Gladwell http://bit.ly/j9o50', 'Highly recommend http://tinyurl.com/HowDavidBeatsGoliath Malcolm Gladwell', 'Blink malcolm gladwell amazing book tipping point', 'Malcolm Gladwell might new man crush', 'omg commercials alone ESPN going drive nuts', '@robmalon Playing Twitter API sounds fun May need take class find new friend like generate results API code', 'playing cURL Twitter API', 'Hello Twitter API ;)', 'playing Java Twitter API', \"@morind45 twitter api slow client's good\", 'yahoo answers butt sometimes', 'scrapbooking Nic =D', 'RT @mashable Five Things Wolfram Alpha Better Vastly Different Google http://bit.ly/6nSnR', 'changed default pic Nike basketball cause bball awesome', 'Nike owns NBA Playoffs ads LeBron Kobe Carmelo http://ow.ly/7Uiy #Adidas #Billups #Howard #Marketing #Branding', \"Next time I'll call Nike\", 'New blog post Nike SB Dunk Low Premium White Gum http://tr.im/lOtT', 'RT @SmartChickPDX told Nike layoffs started today :-(', 'Back worked Nike one fav word :)', \"way I'm totally inspired freaky Nike commercial http://snurl.com/icgj9\"]\n"
     ]
    }
   ],
   "source": [
    "classes = ['4', '2', '0']\n",
    "filename = 'testdata.manual.2009.06.14.csv'\n",
    "filename2 = 'prueba.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "dataset3 = pd.read_csv(filename)\n",
    "tweetys = dataset['final']\n",
    "prueba = pd.read_csv(filename) #para la última parte del NER combinado\n",
    "dataset2 = pd.read_csv(filename2)\n",
    "print(\"1. Carga de archivos realizada\")\n",
    "\n",
    "#LIMPIEZA DE DATOS DE DATASET\n",
    "#TOKENIZATION\n",
    "dataset['tokens'] = dataset['final'].apply(TweetTokenizer().tokenize)\n",
    "#STOPWORDS\n",
    "stopwords_vocabulary = stopwords.words('english') #estará en español?\n",
    "dataset['stopwords'] = dataset['tokens'].apply(lambda x: [i for i in x if i.lower() not in stopwords_vocabulary])\n",
    "#SPECIAL CHARACTERS AND STOPWORDS REMOVAL\n",
    "punctuations = list(string.punctuation)\n",
    "dataset['punctuation'] = dataset['stopwords'].apply(lambda x: [i for i in x if i not in punctuations])\n",
    "dataset['digits'] = dataset['punctuation'].apply(lambda x: [i for i in x if i[0] not in list(string.digits)])\n",
    "dataset['final'] = dataset['digits'].apply(lambda x: [i for i in x if len(i) > 1])\n",
    "print(\"2. Limpieza del dataset realizada\")\n",
    "\n",
    "#LIMPIEZA DE DATOS DE DATASET2\n",
    "#TOKENIZATION\n",
    "dataset2['tokens'] = dataset2['final'].apply(TweetTokenizer().tokenize)\n",
    "#STOPWORDS\n",
    "stopwords_vocabulary = stopwords.words('english') #estará en español?\n",
    "dataset2['stopwords'] = dataset2['tokens'].apply(lambda x: [i for i in x if i.lower() not in stopwords_vocabulary])\n",
    "#SPECIAL CHARACTERS AND STOPWORDS REMOVAL\n",
    "punctuations = list(string.punctuation)\n",
    "dataset2['punctuation'] = dataset2['stopwords'].apply(lambda x: [i for i in x if i not in punctuations])\n",
    "dataset2['digits'] = dataset2['punctuation'].apply(lambda x: [i for i in x if i[0] not in list(string.digits)])\n",
    "dataset2['final'] = dataset2['digits'].apply(lambda x: [i for i in x if len(i) > 1])\n",
    "print(\"3. Limpieza del dataset2 realizada\")\n",
    "\n",
    "#Aquí es el lugar donde defino el número de tweets que usaré en los modelos siempre con el porcentaje 80:20\n",
    "train_data = dataset2['final'][0:300]\n",
    "train_labels = dataset2['label'][0:300]\n",
    "\n",
    "test_data = dataset['final'][0:75]\n",
    "test_labels = dataset['label'][0:75]\n",
    "\n",
    "train_data = list(train_data.apply(' '.join))\n",
    "test_data = list(test_data.apply(' '.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [@stellargirl, loooooooovvvvvveee, Kindle, DX,...\n",
       "1      [Reading, kindle, ..., Love, ..., Lee, childs,...\n",
       "2      [Ok, first, assesment, #kindle2, ..., fucking,...\n",
       "3      [@kenburbary, love, Kindle, I've, mine, months...\n",
       "4      [@mikefish, Fair, enough, Kindle, think, perfe...\n",
       "5       [@richardebaker, big, I'm, quite, happy, Kindle]\n",
       "6      [Fuck, economy, hate, aig, non, loan, given, a...\n",
       "7                            [Jquery, new, best, friend]\n",
       "8                                       [Loves, twitter]\n",
       "9                            [love, Obama, makes, jokes]\n",
       "10     [Check, video, President, Obama, White, House,...\n",
       "11     [@Karoli, firmly, believe, Obama, Pelosi, ZERO...\n",
       "12     [House, Correspondents, dinner, last, night, w...\n",
       "13     [Watchin, Espn, .., Jus, seen, new, Nike, Comm...\n",
       "14     [dear, nike, stop, flywire, shit, waste, scien...\n",
       "15     [#lebron, best, athlete, generation, time, bas...\n",
       "16     [talking, guy, last, night, telling, die, hard...\n",
       "17                   [love, lebron, http://bit.ly/PdHur]\n",
       "18     [@ludajuice, Lebron, Beast, I'm, still, cheeri...\n",
       "19                              [@Pmillzz, lebron, BOSS]\n",
       "20     [@sketchbug, Lebron, hometown, hero, lol, love...\n",
       "21                      [lebron, zydrunas, awesome, duo]\n",
       "22     [@wordwhizkid, Lebron, beast, ..., nobody, NBA...\n",
       "23     [downloading, apps, iphone, much, fun, :-), li...\n",
       "24     [good, news, call, Visa, office, saying, every...\n",
       "25     [http://twurl.nl/epkr4b, awesome, come, back, ...\n",
       "26               [montreal, long, weekend, Much, needed]\n",
       "27     [Booz, Allen, Hamilton, bad, ass, homegrown, s...\n",
       "28     [#MLUC09, Customer, Innovation, Award, Winner,...\n",
       "29     [@SoChi2, current, use, Nikon, D90, love, much...\n",
       "                             ...                        \n",
       "468    [Man, kinda, dislike, Apple, right, Case, poin...\n",
       "469    [@cwong08, Kindle, Sony, PRS, Like, Physical, ...\n",
       "470    [#Kindle2, seems, best, eReader, work, UK, get...\n",
       "471    [google, addiction, Thank, pointing, @annamart...\n",
       "472    [@ruby_gem, primary, debit, card, Visa, Electron]\n",
       "473               [bank, get, new, visa, platinum, card]\n",
       "474    [dearest, @google, rich, bastards, VISA, card,...\n",
       "475       [date, bobby, flay, gut, fieri, food, network]\n",
       "476    [Excited, seeing, Bobby, Flay, Guy, Fieri, tom...\n",
       "477    [Gonna, go, see, Bobby, Flay, Shoreline, Eat, ...\n",
       "478    [can't, wait, great, american, food, music, fe...\n",
       "479    [dad, NY, day, ate, MESA, grill, last, night, ...\n",
       "480                               [Fighting, LaTex, ...]\n",
       "481      [@Iheartseverus, love, want, die, Latex, devil]\n",
       "482    [hours, hours, inkscape, crashing, normally, s...\n",
       "483    [Track, Iran, Social, Media, http://bit.ly/2BoqU]\n",
       "484    [Shit's, hitting, fan, Iran, ..., craziness, i...\n",
       "485    [Monday, already, Iran, may, implode, Kitchen,...\n",
       "486    [Twitter, Stock, buzz, AAPL, ES_F, SPY, SPX, P...\n",
       "487    [getting, ready, test, burger, receipes, weeke...\n",
       "488                  [@johncmayer, Bobby, Flay, joining]\n",
       "489    [lam, love, Bobby, Flay, ..., favorite, RT, @t...\n",
       "490    [created, first, LaTeX, file, scratch, work, w...\n",
       "491    [using, Linux, loving, much, nicer, windows, ....\n",
       "492    [using, LaTeX, lot, typeset, mathematics, look...\n",
       "493    [Ask, Programming, LaTeX, InDesign, submitted,...\n",
       "494    [note, hate, Word, hate, Pages, hate, LaTeX, s...\n",
       "495    [Ahhh, ..., back, real, text, editing, environ...\n",
       "496    [Trouble, Iran, see, Hmm, Iran, Iran, far, awa...\n",
       "497    [Reading, tweets, coming, Iran, ..., whole, th...\n",
       "Name: final, Length: 498, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de los datos para los modelos\n",
    "se prepara la información para dejarla de manera adecuada lista para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=True)\n",
    "#min_df = 5. Descarta palabras que aparezcan en menos de 5 tweets.\n",
    "#max_df = 0.8 Descarta palabras que aparezcan en más del 80% de los tweets\n",
    "#sublinear_tf = True Le da una importancia logarítmica a la repetición de palabras\n",
    "#use_idf = True Permite la frecuencia inversa del documento ???\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 30)\t0.7566258638880964\n",
      "  (1, 18)\t0.6538480726404199\n",
      "  (3, 58)\t0.4193979846865096\n",
      "  (3, 39)\t0.4329765957271401\n",
      "  (3, 36)\t0.4193979846865096\n",
      "  (3, 35)\t0.4076356567863411\n",
      "  (3, 30)\t0.4076356567863411\n",
      "  (3, 0)\t0.3583409983061691\n",
      "  (4, 51)\t1.0\n",
      "  (6, 21)\t1.0\n",
      "  (7, 36)\t1.0\n",
      "  (9, 30)\t1.0\n",
      "  (10, 25)\t0.6854790724168764\n",
      "  (10, 24)\t0.7280923301879362\n",
      "  (11, 60)\t1.0\n",
      "  (12, 37)\t0.5641784811678414\n",
      "  (12, 24)\t0.6149014273164514\n",
      "  (12, 19)\t0.5509980726566459\n",
      "  (13, 36)\t1.0\n",
      "  (14, 30)\t1.0\n",
      "  (15, 60)\t0.5982372933425959\n",
      "  (15, 53)\t0.5982372933425959\n",
      "  (15, 14)\t0.5331268908132939\n",
      "  (16, 37)\t0.6760624044086405\n",
      "  (16, 20)\t0.7368443698266329\n",
      "  :\t:\n",
      "  (56, 25)\t0.6854790724168764\n",
      "  (56, 6)\t0.7280923301879362\n",
      "  (58, 36)\t0.6957540575129336\n",
      "  (58, 31)\t0.7182800926200652\n",
      "  (59, 17)\t1.0\n",
      "  (60, 49)\t0.4369104893054235\n",
      "  (60, 36)\t0.42320850713734204\n",
      "  (60, 35)\t0.4113393103055759\n",
      "  (60, 28)\t0.3752987577286883\n",
      "  (60, 13)\t0.42320850713734204\n",
      "  (60, 12)\t0.3752987577286883\n",
      "  (64, 18)\t1.0\n",
      "  (67, 25)\t0.6854790724168764\n",
      "  (67, 3)\t0.7280923301879362\n",
      "  (69, 25)\t1.0\n",
      "  (70, 53)\t1.0\n",
      "  (71, 36)\t0.5975629417821954\n",
      "  (71, 26)\t0.552797814591363\n",
      "  (71, 25)\t0.5808038453657324\n",
      "  (72, 54)\t1.0\n",
      "  (73, 39)\t0.7703807401589301\n",
      "  (73, 0)\t0.6375841240120231\n",
      "  (74, 61)\t0.5886049790075429\n",
      "  (74, 25)\t0.5541555353644498\n",
      "  (74, 6)\t0.5886049790075429\n"
     ]
    }
   ],
   "source": [
    "print(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para Navie Bayes\n",
    "Según el ejemplo del libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "#nb.fit(train_vectors, train_labels).score(test_vectors, test_labels)\n",
    "#Para guardar el entrenamiento del modelo\n",
    "#file = open('NaiveBayes', 'wb')\n",
    "#dump(nb, file)\n",
    "#Para cargar el entrenamiento del modelo\n",
    "file = open('NaiveBayes', 'rb')\n",
    "nb = load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.33      0.40        21\n",
      "          2       0.21      0.57      0.30        14\n",
      "          4       0.55      0.30      0.39        40\n",
      "\n",
      "avg / total       0.47      0.36      0.37        75\n",
      "\n",
      "[[ 7  9  5]\n",
      " [ 1  8  5]\n",
      " [ 6 22 12]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 0.49333333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes\")\n",
    "print(classification_report(test_labels, nb.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, nb.predict(test_vectors)))\n",
    "predicted = cross_val_predict(nb, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 0 2 2 4 2 0 2 2 4 4 4 4 2 2 4 2 2 0 0 0 0 2 4 4 2 4 0 2 4 2 2\n",
      " 2 4 2 0 2 2 0 0 2 2 0 4 2 0 2 2 2 4 4 4 2 4 2 2 2 2 2 0 2 2 4 2 4 4 4 0 4\n",
      " 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29cb3dd4400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = nb.predict(test_vectors)\n",
    "print(result)\n",
    "pos = len(result[result == 4]) #guardamos la cantidad de tweets positivos\n",
    "neg = len(result[result == 0]) #guardamos la cantidad de tweets negativos\n",
    "neu = len(result[result == 2]) #guardamos la cantidad de tweets neutros\n",
    "y = [pos, neu, neg] # vector de la cantidad de tweets positivos, negativos y neutros\n",
    "#construimos un gráfico con los datos del vector\n",
    "plt.title(\"Sentiment Analysis\")\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xticks(range(len(y)), ['positive', 'neutral', 'negative'])\n",
    "plt.bar(range(len(y)), height=y, width = 0.75, align = 'center', alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msvc = SVC()\n",
    "msvc.fit(train_vectors, train_labels).score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      1.00      0.44        21\n",
      "          2       0.00      0.00      0.00        14\n",
      "          4       0.00      0.00      0.00        40\n",
      "\n",
      "avg / total       0.08      0.28      0.12        75\n",
      "\n",
      "[[21  0  0]\n",
      " [14  0  0]\n",
      " [40  0  0]]\n",
      "Cross validation 0.5333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVC\")\n",
    "print(classification_report(test_labels, msvc.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, msvc.predict(test_vectors)))\n",
    "predicted = cross_val_predict(msvc, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier()\n",
    "neigh.fit(train_vectors, train_labels).score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.52      0.32        21\n",
      "          2       0.15      0.21      0.18        14\n",
      "          4       0.50      0.10      0.17        40\n",
      "\n",
      "avg / total       0.36      0.24      0.21        75\n",
      "\n",
      "[[11  8  2]\n",
      " [ 9  3  2]\n",
      " [27  9  4]]\n",
      "Cross validation 0.37333333333333335\n"
     ]
    }
   ],
   "source": [
    "print(\"KNeighbors Classifier\")\n",
    "print(classification_report(test_labels, neigh.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, neigh.predict(test_vectors)))\n",
    "predicted = cross_val_predict(neigh, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para DessisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3466666666666667"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(train_vectors, train_labels).score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors Classifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.33      0.40        21\n",
      "          2       0.20      0.57      0.29        14\n",
      "          4       0.55      0.28      0.37        40\n",
      "\n",
      "avg / total       0.47      0.35      0.36        75\n",
      "\n",
      "[[ 7 10  4]\n",
      " [ 1  8  5]\n",
      " [ 6 23 11]]\n",
      "Cross validation 0.49333333333333335\n"
     ]
    }
   ],
   "source": [
    "print(\"DecisionTreeClassifier\")\n",
    "print(classification_report(test_labels, tree.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, tree.predict(test_vectors)))\n",
    "predicted = cross_val_predict(tree, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo para MLP Perceptrion Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(train_vectors, train_labels).score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.29      0.36        21\n",
      "          2       0.19      0.57      0.29        14\n",
      "          4       0.52      0.28      0.36        40\n",
      "\n",
      "avg / total       0.45      0.33      0.35        75\n",
      "\n",
      "[[ 6 10  5]\n",
      " [ 1  8  5]\n",
      " [ 5 24 11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 0.5066666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"MLPClassifier\")\n",
    "print(classification_report(test_labels, mlp.predict(test_vectors)))\n",
    "print(confusion_matrix(test_labels, mlp.predict(test_vectors)))\n",
    "predicted = cross_val_predict(mlp, test_vectors, test_labels, cv=10)\n",
    "print(\"Cross validation %s\" % accuracy_score(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Name Entity Recognition\n",
    "En la siguiente sección implementamos el reconocimiento de entidades. Es muy lento, así que vamos a tener que ver cómo acelerar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('GM', 20),\n",
       " ('Time', 20),\n",
       " ('Warner', 20),\n",
       " ('Nike', 16),\n",
       " ('Google', 11),\n",
       " ('Stanford', 10),\n",
       " ('Safeway', 6),\n",
       " ('Canon', 5),\n",
       " ('New', 5),\n",
       " ('AIG', 4)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = StanfordNERTagger(r'C:\\Users\\Servicio Técnico\\Documents\\stanford-ner-2018-02-27\\classifiers\\english.all.3class.distsim.crf.ser.gz')\n",
    "#acuérdate de que cambia para el mac que es donde vas a realizar la presentación\n",
    "entities = []\n",
    "\n",
    "for r in tweetys:\n",
    "    #print(\"está analizando(r): \", r)\n",
    "    lst_tags = st.tag(r) #no tengo que hacer el split porque ya está hecho?\n",
    "    for tup in lst_tags:\n",
    "        #print(\"está analizando(tup): \", tup)\n",
    "        if(tup[1] != 'O'):\n",
    "            #print(\"mete(tup) \", tup, \"en las entidades\")\n",
    "            entities.append(tup)\n",
    "df_entities = pd.DataFrame(entities)\n",
    "df_entities.columns = [\"word\",\"ner\"]\n",
    "#Organizaciones\n",
    "organizations =df_entities[df_entities['ner'].str.contains(\"ORGANIZATION\")]\n",
    "cnt = Counter(organizations['word'])\n",
    "cnt.most_common(10)\n",
    "\n",
    "\n",
    "# 1. Coger datos de twitter.\n",
    "# 2. Cargar un csv con datos y verlo en una gráfica y usar NER.\n",
    "# 3. Tweet por tweet determinar de qué está hablando y el sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Obama', 11),\n",
       " ('Malcolm', 7),\n",
       " ('Warren', 7),\n",
       " ('Bobby', 7),\n",
       " ('Gladwell', 6),\n",
       " ('Flay', 6),\n",
       " ('Pelosi', 5),\n",
       " ('Buffet', 5),\n",
       " ('Kobe', 4),\n",
       " ('Booz', 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Personas\n",
    "person =df_entities[df_entities['ner'].str.contains(\"PERSON\")]\n",
    "cnt_person = Counter(person['word'])\n",
    "cnt_person.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('China', 9),\n",
       " ('San', 8),\n",
       " ('Francisco', 8),\n",
       " ('North', 6),\n",
       " ('Korea', 6),\n",
       " ('Iran', 4),\n",
       " ('Bay', 2),\n",
       " ('East', 2),\n",
       " ('Palo', 2),\n",
       " ('Alto', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Localizaciones\n",
    "locations =df_entities[df_entities['ner'].str.contains(\"LOCATION\")]\n",
    "cnt_location = Counter(locations['word'])\n",
    "cnt_location.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinando NER con el análisis de sentimientos.\n",
    "Lo que vamos a hacer ahora es combinar el reconocimiento de entidades con el análisis de sentimientos. En el libro lo pone de manera explícita, pero vamos a tratar de hacerlo de manera automática.\n",
    "Es decir vamos a coger el primero de todas las organizaciones, personas y lugares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.142857142857143"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = prueba[prueba['final'].str.contains(cnt_person.most_common(2)[1][0])]\n",
    "avg_sentiment = np.mean(subset['label'])\n",
    "avg_sentiment\n",
    "subset['label']\n",
    "#subset = dataset[dataset['final'].str.contains(cnt_person.most_common(1)[0][0])]\n",
    "#avg_sentiment = np.mean(subset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a():\n",
    "    x = (1,9)\n",
    "    n = (2,3)\n",
    "    m = ('h','r')\n",
    "    return (x,n,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 9), (2, 3), ('h', 'r'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = a()\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahora toca  1\n",
      "ahora toca  9\n",
      "ahora toca  2\n",
      "ahora toca  3\n",
      "ahora toca  h\n",
      "ahora toca  r\n"
     ]
    }
   ],
   "source": [
    "for x in resultado:\n",
    "    for y in x:\n",
    "        print('ahora toca ',y)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
